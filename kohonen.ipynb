{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Organising Map Challenge\n",
    "\n",
    "## The Kohonen Network\n",
    "\n",
    "The Kohonen Self Organising Map (SOM) provides a data visualization technique which helps to understand high dimensional data by reducing the dimensions of data to a map. SOM also represents clustering concept by grouping similar data together.\n",
    "\n",
    "Unlike other learning technique in neural networks, training a SOM requires no target vector. A SOM learns to classify the training data without any external supervision.\n",
    "\n",
    "![Network](http://www.pitt.edu/~is2470pb/Spring05/FinalProjects/Group1a/tutorial/kohonen1.gif)\n",
    "\n",
    "### Structure\n",
    "A network has a width and a height that descibes the grid of nodes.  For example, the grid may be 4x4, and so there would be 16 nodes.\n",
    "\n",
    "Each node has a weight for each value in the input vector.  A weight is simply a float value that the node multiplies the input value by to determine how influential it is (see below)\n",
    "\n",
    "Each node has a set of weights that match the size of the input vector.  For example, if the input vector has 10 elements, each node would have 10 weights.\n",
    "\n",
    "### Training \n",
    "To train the network\n",
    "\n",
    "1. Each node's weights are initialized.\n",
    "2. We enumerate through the training data for some number of iterations (repeating if necessary).  The current value we are training against will be referred to as the `current input vector`\n",
    "3. Every node is examined to calculate which one's weights are most like the input vector. The winning node is commonly known as the Best Matching Unit (BMU).\n",
    "4. The radius of the neighbourhood of the BMU is now calculated. This is a value that starts large, typically set to the 'radius' of the lattice,  but diminishes each time-step. Any nodes found within this radius are deemed to be inside the BMU's neighbourhood.\n",
    "5. Each neighbouring node's (the nodes found in step 4) weights are adjusted to make them more like the input vector. The closer a node is to the BMU, the more its weights get altered.\n",
    "6. Go to step 2 until we've completed N iterations.\n",
    "    \n",
    "\n",
    "### Calculating the Best Matching Unit (BMU)\n",
    "\n",
    "To determine the best matching unit, one method is to iterate through all the nodes and calculate the Euclidean distance between each node's weight vector and the current input vector. The node with a weight vector closest to the input vector is tagged as the BMU.\n",
    "\n",
    "The Euclidean distance $\\mathsf{distance}_{i}$ (from the input vector $V$ to the $i$th node's weights $W_i$)is given as (using Pythagoras):\n",
    "\n",
    "$$ \\mathsf{distance}_{i}=\\sqrt{\\sum_{k=0}^{k=n}(V_k - W_{i_k})^2}$$\n",
    "\n",
    "where V is the current input vector and $W_i$ is the node's weight vector.  $n$ is the size of the input & weight vector.\n",
    "\n",
    "*Note*: $V$ and $W$ are vectors.  $V$ is the input vector, and $W_i$ is the weight vector of the $i$th node.  $V_k$ and $W_{i_k}$ represent the $k$'th value within those vectors.  \n",
    "\n",
    "The BMU is the node with the minimal distance for the current input vector\n",
    "\n",
    "### Calculating the Neighbourhood Radius\n",
    "\n",
    "The next step is to calculate which of the other nodes are within the BMU's neighbourhood. All these nodes will have their weight vectors altered.\n",
    "\n",
    "First we calculate what the radius of the neighbourhood should be and then use Pythagoras to determine if each node is within the radial distance or not.\n",
    "\n",
    "A unique feature of the Kohonen learning algorithm is that the area of the neighbourhood shrinks over time. To do this we use the exponential decay function:\n",
    "\n",
    "Given a desired number of training iterations $n$:\n",
    "$$n_{\\mathsf{max iterations}} = 100$$\n",
    "\n",
    "Calculate the radius $\\sigma_t$ at iteration number $t$:\n",
    "\n",
    "$$\\sigma_t = \\sigma_0 \\exp\\left(- \\frac{t}{\\lambda} \\right) \\qquad t = 1,2,3,4... $$\n",
    "\n",
    "Where $\\sigma_0$ denotes the neighbourhood radius at iteration $t=0$, $t$ is the current iteration. We define $\\sigma_0$ (the initial radius) and $\\lambda$ (the time constant) as below:\n",
    "\n",
    "$$\\sigma_0 = \\frac{\\max(width,height)}{2} \\qquad \\lambda = \\frac{n_{\\mathsf{max iterations}}}{\\log(\\sigma_0)} $$\n",
    "\n",
    "Where $width$ & $height$ are the width and height of the grid.\n",
    "\n",
    "### Calculating the Learning Rate\n",
    "\n",
    "We define the initial leanring rate $\\alpha_0$ at iteration $t = 0$ as:\n",
    "$$\\alpha_0 = 0.1$$\n",
    "\n",
    "So, we can calculate the learning rate at a given iteration t as:\n",
    "\n",
    "$$\\alpha_t = \\alpha_0 \\exp \\left(- \\frac{t}{\\lambda} \\right) $$\n",
    "\n",
    "where $t$ is the iteration number, $\\lambda$ is the time constant (calculated above)\n",
    "        \n",
    "### Calculating the Influence\n",
    "\n",
    "As well as the learning rate, we need to calculate the influence $\\theta_t$ of the learning/training at a given iteration $t$.  \n",
    "\n",
    "So for each node, we need to caclulate the euclidean distance $d_i$ from the BMU to that node.  Similar to when we calculate the distance to find the BMU, we use Pythagoras.  The current ($i$th) node's x position is given by $x(W_i)$, and the BMU's x position is, likewise, given by $x(Z)$.  Similarly, $y()$ returns the y position of a node.\n",
    "\n",
    "$$ d_{i}=\\sqrt{(x(W_i) - x(Z))^2 + (y(W_i) - y(Z))^2} $$\n",
    "\n",
    "Then, the influence decays over time according to:\n",
    "\n",
    "$$\\theta_t = \\exp \\left( - \\frac{d_{i}^2}{2\\sigma_t^2} \\right) $$\n",
    "\n",
    "Where $\\sigma_t$ is the neighbourhood radius at iteration $t$ as calculated above. \n",
    "\n",
    "Note: You will need to come up with an approach to x() and y().\n",
    "\n",
    "\n",
    "### Updating the Weights\n",
    "\n",
    "To update the weights of a given node, we use:\n",
    "\n",
    "$$W_{i_{t+1}} = W_{i_t} + \\alpha_t \\theta_t (V_t - W_{i_t})$$\n",
    "        \n",
    "So $W_{i_{t+1}}$ is the new value of the weight for the $i$th node, $V_t$ is the current value of the training data, $W_{i_t}$ is the current weight and $\\alpha_t$ and $\\theta_t$ are the learning rate and influence calculated above.\n",
    "\n",
    "*Note*: the $W$ and $V$ are vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Sam has written an implementation of a Self Organising Map. Consider the following criteria when assessing Sam's code:\n",
    "\n",
    "- Could the code be made more efficient? A literal interpretation of the instructions above is not necessary.\n",
    "  - Using too many python for loops. Must us numpy and vectors. This will also allow to use GPU in case that is available **Saad**\n",
    "- Is the code best structured for later use by other developers and in anticipation of productionisation?\n",
    "  - Nope, with this code it is very hard to understand what is happening at a glance\n",
    "    - Make a class\n",
    "    - Make internal functions for BMU, neighbourhood radius, learning rate and influence\n",
    "- How would you approach productionising this application?\n",
    "- Anything else you think is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kohonen.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(input_data, n_max_iterations, width, height, init_weights=None):\n",
    "    σ0 = max(width, height) / 2\n",
    "    α0 = 0.1\n",
    "    weights = (\n",
    "        init_weights\n",
    "        if init_weights is not None\n",
    "        else np.random.random((width, height, 3))\n",
    "    )\n",
    "    λ = n_max_iterations / np.log(σ0)\n",
    "    for t in range(n_max_iterations):\n",
    "        σt = σ0 * np.exp(-t / λ)\n",
    "        αt = α0 * np.exp(-t / λ)\n",
    "        for vt in input_data:\n",
    "            bmu = np.argmin(np.sum((weights - vt) ** 2, axis=2))\n",
    "            bmu_x, bmu_y = np.unravel_index(bmu, (width, height))\n",
    "            for x in range(width):\n",
    "                for y in range(height):\n",
    "                    di = np.sqrt(((x - bmu_x) ** 2) + ((y - bmu_y) ** 2))\n",
    "                    θt = np.exp(-(di**2) / (2 * (σt**2)))\n",
    "                    weights[x, y] += αt * θt * (vt - weights[x, y])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing efficiency \n",
    "\n",
    "- Use vector broad casting\n",
    "- numpy vector operations instead of loops\n",
    "- Update can be done without looping for x and y with meshgrid and broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_train(input_data, n_max_iterations, width, height, init_weights=None):\n",
    "    init_learning_rate = 0.1\n",
    "    init_radius = max(width, height) / 2\n",
    "    time_constant = n_max_iterations / np.log(init_radius)\n",
    "    \n",
    "    weights = (\n",
    "        init_weights\n",
    "        if init_weights is not None\n",
    "        else np.random.random((width, height, input_data.shape[-1]))\n",
    "    )\n",
    "    coord_x, coord_y = np.meshgrid(np.arange(width), np.arange(height), indexing=\"ij\")\n",
    "    for t in range(n_max_iterations):\n",
    "        current_radius = init_radius * np.exp(-t / time_constant)\n",
    "        current_learning_rate = init_learning_rate * np.exp(-t / time_constant)\n",
    "        for vt in input_data:\n",
    "            bmu = np.argmin(np.sum((weights - vt) ** 2, axis=2))\n",
    "            bmu_x, bmu_y = np.unravel_index(bmu, (width, height))\n",
    "\n",
    "            influence = np.sqrt((coord_x - bmu_x) ** 2 + (coord_y - bmu_y) ** 2)\n",
    "            influence_decay = np.exp(-(influence**2) / (2 * current_radius**2))\n",
    "            # broadcasting\n",
    "            influence_decay = influence_decay.reshape(\n",
    "                influence_decay.shape + (1,) * (weights.ndim - influence_decay.ndim)\n",
    "            )\n",
    "\n",
    "            weights += current_learning_rate * influence_decay * (vt - weights)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production worth - Kohonen Map\n",
    "\n",
    "- Logger class\n",
    "- Kohonen Map class\n",
    "- Model registry\n",
    "- Build, retraining, and inference pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger\n",
    "\n",
    "A `Telemetry` class which can log, logs to a jsonl file. It is a helper class, for the telemetry, so no need to look too much at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, logging, json\n",
    "\n",
    "class Telemetry:\n",
    "    def __init__(self, level=logging.INFO, log_path='.logs/telemetry.log'):\n",
    "        \"\"\"Initializes the telemetry system with a logger.\"\"\"\n",
    "        self.__logger = logging.getLogger('TelemetryLogger')\n",
    "        self.__logger.setLevel(level)\n",
    "        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "        handler = logging.FileHandler(log_path)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "        handler.setFormatter(formatter)\n",
    "        self.__logger.addHandler(handler)\n",
    "\n",
    "    def log(self, message: str, level=logging.INFO, **kwargs):\n",
    "        \"\"\"Logs a message with a given level.\"\"\"\n",
    "        self.__logger.log(level, message, extra=kwargs)\n",
    "\n",
    "    def error(self, error: Exception):\n",
    "        self.__logger(logging.ERROR, f\"[Training Error] f{str(error)}\", extra=dict(error=error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production worthy Kohonen Maps\n",
    "\n",
    "- Make is a class\n",
    "- Attach telemetry\n",
    "- The private methods must tell the steps in the algorithm\n",
    "- Use numpy boradcasts and vector algorithms for faster iterations\n",
    "- Add inference endpoint.\n",
    "- Add versioning for model code, param change, and retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Callable\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import os, pickle\n",
    "\n",
    "\n",
    "class KohonenMap:\n",
    "    \"\"\"\n",
    "    Implementation of a Kohonen Self-Organizing Map (SOM).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        width: int,\n",
    "        height: int,\n",
    "        input_dim: int,\n",
    "        model_version: int = 0, # Model build/ code version\n",
    "        model_last_revision: int = 0, # Model training version\n",
    "        max_iterations: int = 1000,\n",
    "        learning_rate: float = 0.1,\n",
    "        weights: Optional[np.ndarray] = None,\n",
    "        telemetry: Telemetry = Telemetry(),\n",
    "        checkpoint_dir: str = './checkpoints'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Kohonen Map with the given dimensions and training parameters.\n",
    "\n",
    "        :param width: Width of the map.\n",
    "        :param height: Height of the map.\n",
    "        :param input_dim: Number of dimensions of the input vectors.\n",
    "        :param max_iterations: Maximum number of iterations for training.\n",
    "        :param learning_rate: Initial learning rate.\n",
    "        :param weights: Initial weights of the SOM. Randomly initialized if None.\n",
    "        :param telemetry: Telemetry object for logging.\n",
    "        :param checkpoint_dir: Directory where checkpoints will be saved.\n",
    "        \"\"\"\n",
    "        self.model_code_version: int = 0 # Update this when model code changes\n",
    "        self.model_version = model_version\n",
    "        self.model_last_revision = model_last_revision\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = (\n",
    "            weights\n",
    "            if weights is not None\n",
    "            else np.random.random((width, height, input_dim))\n",
    "        )\n",
    "        assert self.weights.shape == (\n",
    "            width,\n",
    "            height,\n",
    "            input_dim,\n",
    "        ), f\"The weights must be of shape {(width, height, input_dim)}. The given is {self.weights.shape}\"\n",
    "        self.max_iterations = max_iterations\n",
    "        self.meshgrid = np.meshgrid(np.arange(width), np.arange(height), indexing=\"ij\")\n",
    "        self.init_radius = max(width, height) / 2\n",
    "        self.time_constant = max_iterations / np.log(self.init_radius)\n",
    "        self.telemetry = telemetry\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def __get_bmu(self, vector: np.ndarray) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Identifies the best matching unit (BMU) for a given input vector.\n",
    "\n",
    "        :param vector: Input vector.\n",
    "        :return: Tuple of indices for the BMU.\n",
    "        \"\"\"\n",
    "        self.telemetry.log(\n",
    "            \"Entered __get_bmu function\", logging.DEBUG, input=dict(vector=vector)\n",
    "        )\n",
    "        bmu = np.argmin(np.sum((self.weights - vector) ** 2, axis=2))\n",
    "        return np.unravel_index(bmu, (self.width, self.height))\n",
    "\n",
    "    def __calculate_influence(\n",
    "        self, bmu: Tuple[int, int], current_radius: float\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates the influence of the BMU over the map's neurons.\n",
    "\n",
    "        :param bmu: Best matching unit (BMU) indices.\n",
    "        :param current_radius: Current neighborhood radius.\n",
    "        :return: Influence matrix.\n",
    "        \"\"\"\n",
    "        self.telemetry.log(\n",
    "            \"Entered __calculate_influence function\",\n",
    "            logging.DEBUG,\n",
    "            input=dict(bmu=bmu, current_radius=current_radius),\n",
    "        )\n",
    "        bmu_x, bmu_y = bmu\n",
    "        coord_x, coord_y = self.meshgrid\n",
    "        influence = np.sqrt((coord_x - bmu_x) ** 2 + (coord_y - bmu_y) ** 2)\n",
    "        influence_decay = np.exp(-(influence**2) / (2 * current_radius**2))\n",
    "        return influence_decay.reshape(\n",
    "            influence_decay.shape + (1,) * (self.weights.ndim - influence_decay.ndim)\n",
    "        )\n",
    "\n",
    "    def __save_checkpoint(self, iteration: int):\n",
    "        \"\"\"\n",
    "        Saves a checkpoint of the current model state.\n",
    "\n",
    "        :param iteration: The current training iteration.\n",
    "        \"\"\"\n",
    "        checkpoint_file = os.path.join(\n",
    "            self.checkpoint_dir, f\"checkpoint_{self.model_code_version}.{self.model_version}.{self.model_last_revision}.{iteration}.pkl\"\n",
    "        )\n",
    "        checkpoint_data = {\n",
    "            \"iteration\": iteration,\n",
    "            \"weights\": self.weights,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"model_code_version\": self.model_code_version,\n",
    "            \"model_version\": self.model_version,\n",
    "            \"model_last_revision\": self.model_last_revision,\n",
    "        }\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(checkpoint_data, f)\n",
    "        self.telemetry.log(f\"Checkpoint saved at iteration {iteration}\", logging.INFO)\n",
    "\n",
    "    def __load_checkpoint(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Loads the most recent checkpoint if available.\n",
    "\n",
    "        :return: The iteration number from the checkpoint or None if no checkpoint exists.\n",
    "        \"\"\"\n",
    "        checkpoint_files = [f for f in os.listdir(self.checkpoint_dir) if f.startswith(\"checkpoint_\")]\n",
    "        if not checkpoint_files:\n",
    "            return None\n",
    "\n",
    "        latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "        checkpoint_file = os.path.join(self.checkpoint_dir, latest_checkpoint)\n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            checkpoint_data = pickle.load(f)\n",
    "        \n",
    "        self.weights = checkpoint_data[\"weights\"]\n",
    "        self.learning_rate = checkpoint_data[\"learning_rate\"]\n",
    "        self.model_code_version = checkpoint_data[\"model_code_version\"]\n",
    "        self.model_version = checkpoint_data[\"model_version\"]\n",
    "        self.model_last_revision = checkpoint_data[\"model_last_revision\"]\n",
    "        \n",
    "        iteration = checkpoint_data[\"iteration\"]\n",
    "        self.telemetry.log(f\"Loaded checkpoint from iteration {iteration}\", logging.INFO)\n",
    "        return iteration\n",
    "\n",
    "    def __delete_checkpoints(self):\n",
    "        \"\"\"\n",
    "        Deletes all checkpoint files in the checkpoint directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            checkpoint_files = [f for f in os.listdir(self.checkpoint_dir) if f.startswith(f\"checkpoint_{self.model_code_version}.{self.model_version}.{self.model_last_revision}\")]\n",
    "            for file_name in checkpoint_files:\n",
    "                os.remove(os.path.join(self.checkpoint_dir, file_name))\n",
    "            self.telemetry.log(\"All checkpoints have been deleted.\", logging.INFO)\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "\n",
    "    def __train(self, input_data: np.ndarray, training_checkpoint: int):\n",
    "        \"\"\"\n",
    "        Trains the Kohonen map using the provided input data.\n",
    "\n",
    "        :param input_data: Input data array.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            len(input_data.shape) == 2 and input_data.shape[-1] == self.input_dim\n",
    "        ), f\"The input_data must be of shape (N, {self.input_dim}). The given is {input_data.shape}\"\n",
    "\n",
    "        # Resume iterations just in case\n",
    "        start_iteration = self.__load_checkpoint() or 0\n",
    "\n",
    "        for iteration in tqdm(\n",
    "            range(self.max_iterations),\n",
    "            total=self.max_iterations,\n",
    "            initial=start_iteration,\n",
    "            desc=\"Kohonen fitting iterations\",\n",
    "        ):\n",
    "            start_time = time.time()\n",
    "            ## Kohonene fitting - start\n",
    "            current_radius = self.init_radius * np.exp(-iteration / self.time_constant)\n",
    "            current_learning_rate = self.learning_rate * np.exp(\n",
    "                -iteration / self.time_constant\n",
    "            )\n",
    "            for vector in input_data:\n",
    "                bmu = self.__get_bmu(vector)\n",
    "                influence_decay = self.__calculate_influence(bmu, current_radius)\n",
    "                self.weights += (\n",
    "                    current_learning_rate * influence_decay * (vector - self.weights)\n",
    "                )\n",
    "            ## Kohonen fitting - end\n",
    "            if iteration % training_checkpoint == 0:\n",
    "                self.__save_checkpoint(iteration)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            telemetry_message = f\"Iteration {iteration}/{self.max_iterations} complete in f{elapsed_time:.5f}s\"\n",
    "            self.telemetry.log(telemetry_message)\n",
    "            self.telemetry.log(\n",
    "                telemetry_message,\n",
    "                level=logging.DEBUG,\n",
    "                params=dict(\n",
    "                    iteration=iteration,\n",
    "                    total_iterations=self.max_iterations,\n",
    "                    kohonen_map_size=(self.width, self.height),\n",
    "                    input_data_shape=input_data.shape,\n",
    "                    init_radius=self.init_radius,\n",
    "                    time_constant=self.time_constant,\n",
    "                    current_radius=current_radius,\n",
    "                    current_learning_rate=current_learning_rate,\n",
    "                    elapsed_time=elapsed_time,\n",
    "                ),\n",
    "            )\n",
    "        self.__delete_checkpoints()\n",
    "        return self.weights\n",
    "\n",
    "    def train(self, input_data: np.ndarray, training_checkpoint: int = 100):\n",
    "        \"\"\"\n",
    "        Public method to start the training process.\n",
    "\n",
    "        :param input_data: Input data array.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.telemetry.log(\n",
    "                \"Entered train function\",\n",
    "                level=logging.DEBUG,\n",
    "                input=dict(\n",
    "                    input_data_shape=input_data.shape,\n",
    "                    training_checkpoint=training_checkpoint,\n",
    "                ),\n",
    "            )\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = self.__train(input_data, training_checkpoint)\n",
    "            self.model_last_revision += 1\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            self.telemetry.log(f\"Training completed in {elapsed_time}s\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "            raise e\n",
    "            \n",
    "\n",
    "    def infer(self, input_data: np.ndarray) -> Tuple[int, int, float]:\n",
    "        \"\"\"\n",
    "        Infers the best matching unit (BMU) for a given input vector.\n",
    "\n",
    "        :param input_vector: Input vector to locate the BMU for.\n",
    "        :return: Tuple of indices representing the position of the BMU and the eucleadian distance for the bmu.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.telemetry.log(\n",
    "                f\"Entered infer function with model ({self.model_version}.{self.model_last_revision})\",\n",
    "                level=logging.DEBUG,\n",
    "                input=dict(input_data_shape=input_data.shape),\n",
    "            )\n",
    "            assert len(input_data.shape) == 2 and input_data.shape[-1] == self.input_dim, f'The input vector MUST be of shape (N, {self.input_dim}). The provided is f{input_data.shape}'\n",
    "            result: list[Tuple[int, int, float]] = []\n",
    "            for vector in input_data:\n",
    "                bmu_index = np.argmin(np.linalg.norm(self.weights - vector, axis=2))\n",
    "                bmu_coords = np.unravel_index(bmu_index, (self.width, self.height))\n",
    "                euclidean_distance = np.linalg.norm(self.weights[bmu_coords] - vector)\n",
    "                result.append((*bmu_coords, euclidean_distance))\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Resgistry\n",
    "\n",
    "Store the model based on its version, environment, and revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class KohonenMapRegistry:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        environment: str = 'dev',\n",
    "        artefact_base_name: str = 'kohonen',\n",
    "    ):\n",
    "        # Registery directory\n",
    "        self.__dir_path = os.path.join(dir_path, environment)\n",
    "        self.__environment = environment\n",
    "        self.__artefact_base_name = artefact_base_name\n",
    "        os.makedirs(self.__dir_path, exist_ok=True)\n",
    "\n",
    "    def save(self, kohonenMap: KohonenMap):\n",
    "        version = kohonenMap.model_version\n",
    "        revision = kohonenMap.model_last_revision\n",
    "        code_version = kohonenMap.model_code_version\n",
    "        model_artefact = dict(\n",
    "            width=kohonenMap.width,\n",
    "            height=kohonenMap.height,\n",
    "            input_dim=kohonenMap.input_dim,\n",
    "            max_iterations=kohonenMap.max_iterations,\n",
    "            learning_rate=kohonenMap.learning_rate,\n",
    "            weights=kohonenMap.weights,\n",
    "            build_version=version,\n",
    "            code_version=code_version,\n",
    "            training_revision=revision,\n",
    "            execution_environment=self.__environment,\n",
    "        )\n",
    "        model_artefact_file_name = f\"{self.__artefact_base_name}-{self.__environment}-{code_version}.{version}.{revision}.pkl\"\n",
    "        file_path = os.path.join(self.__dir_path, model_artefact_file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            raise FileExistsError(f\"The file {file_path} already exists.\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(model_artefact, f)\n",
    "        print(f\"Model artefact saved at {file_path}\")\n",
    "        \n",
    "    def load(self, code_version: int, version: int, revision: int) -> KohonenMap:\n",
    "        model_artefact_file_name = f\"{self.__artefact_base_name}-{self.__environment}-{code_version}.{version}.{revision}.pkl\"\n",
    "        file_path = os.path.join(self.__dir_path, model_artefact_file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "        with open(file_path, 'rb') as f:\n",
    "            model_artefact = pickle.load(f)\n",
    "        kohonenMap = KohonenMap(\n",
    "            width=model_artefact['width'],\n",
    "            height=model_artefact['height'],\n",
    "            input_dim=model_artefact['input_dim'],\n",
    "            max_iterations=model_artefact['max_iterations'],\n",
    "            learning_rate=model_artefact['learning_rate'],\n",
    "            weights=model_artefact['weights'],\n",
    "            model_version=version,\n",
    "            model_last_revision=revision\n",
    "        )\n",
    "        print(f\"Model artefact loaded from {file_path}\")\n",
    "        return kohonenMap\n",
    "\n",
    "    def list(self) -> list[Tuple[int, int, int]]:\n",
    "        artefacts = []\n",
    "        for file_name in os.listdir(self.__dir_path):\n",
    "            if file_name.startswith(self.__artefact_base_name) and file_name.endswith('.pkl'):\n",
    "                parts = file_name.split('-')\n",
    "                env, version_revision = parts[1], parts[2]\n",
    "                if env == self.__environment:\n",
    "                    code_version, version, revision = map(int, version_revision.split('.')[0:3])\n",
    "                    artefacts.append((code_version, version, revision))\n",
    "        \n",
    "        artefacts.sort(reverse=True, key=lambda x: (x[0], x[1]))\n",
    "        return artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:  20%|█▉        | 793/4000 [00:09<00:40, 79.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m init_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom((w, h, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimsave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/init\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, init_weights)\n\u001b[0;32m----> 9\u001b[0m image_data \u001b[38;5;241m=\u001b[39m \u001b[43mp_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mimsave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/p\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_data)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# image_data = e_train(input_data, iterations, w, h, init_weights)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# plt.imsave(\"e100.png\", image_data)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# plt.imsave('1000.png', image_data)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 158\u001b[0m, in \u001b[0;36mp_train\u001b[0;34m(input_data, n_max_iterations, width, height, init_weights)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_train\u001b[39m(input_data, n_max_iterations, width, height, init_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    149\u001b[0m     kmap \u001b[38;5;241m=\u001b[39m KohonenMap(\n\u001b[1;32m    150\u001b[0m         width\u001b[38;5;241m=\u001b[39mwidth,\n\u001b[1;32m    151\u001b[0m         height\u001b[38;5;241m=\u001b[39mheight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 158\u001b[0m     \u001b[43mkmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kmap\u001b[38;5;241m.\u001b[39mweights\n",
      "Cell \u001b[0;32mIn[6], line 132\u001b[0m, in \u001b[0;36mKohonenMap.train\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mPublic method to start the training process.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m:param input_data: Input data array.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtelemetry\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    124\u001b[0m     level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, \n\u001b[1;32m    125\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining started\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     time_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_constant,\n\u001b[1;32m    131\u001b[0m )\n\u001b[0;32m--> 132\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__telemtry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtelemetry\u001b[38;5;241m.\u001b[39mlog(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining ended\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mTelemetry.logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG, func_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, result\u001b[38;5;241m=\u001b[39mresult)\n\u001b[1;32m     28\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m, in \u001b[0;36mKohonenMap.__train\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    110\u001b[0m     bmu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__telemtry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_bmu)(vector)            \n\u001b[1;32m    111\u001b[0m     influence_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__telemtry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__calculate_influence)(bmu, current_radius)\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_learning_rate \u001b[38;5;241m*\u001b[39m influence_decay \u001b[38;5;241m*\u001b[39m (\u001b[43mvector\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    w, h = (400, 400)\n",
    "    iterations = 100\n",
    "    input_data = np.random.random((40, 3))\n",
    "    init_weights = np.random.random((w, h, 3))\n",
    "    plt.imsave(f\"images/init{w}.png\", init_weights)\n",
    "    \n",
    "    image_data = p_train(input_data, iterations, w, h, init_weights)\n",
    "    plt.imsave(f\"images/p{w}.png\", image_data)\n",
    "\n",
    "    # image_data = e_train(input_data, iterations, w, h, init_weights)\n",
    "    # plt.imsave(\"e100.png\", image_data)\n",
    "\n",
    "    # image_data = train(input_data, iterations, w, h, init_weights)\n",
    "    # plt.imsave(\"100.png\", image_data)\n",
    "\n",
    "    # Generate data\n",
    "    # input_data = np.random.random((10,3))\n",
    "    # image_data = train(input_data, 1000, 100, 100)\n",
    "\n",
    "    # plt.imsave('1000.png', image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
