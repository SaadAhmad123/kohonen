{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Organising Map Challenge\n",
    "\n",
    "## The Kohonen Network\n",
    "\n",
    "The Kohonen Self Organising Map (SOM) provides a data visualization technique which helps to understand high dimensional data by reducing the dimensions of data to a map. SOM also represents clustering concept by grouping similar data together.\n",
    "\n",
    "Unlike other learning technique in neural networks, training a SOM requires no target vector. A SOM learns to classify the training data without any external supervision.\n",
    "\n",
    "![Network](http://www.pitt.edu/~is2470pb/Spring05/FinalProjects/Group1a/tutorial/kohonen1.gif)\n",
    "\n",
    "### Structure\n",
    "A network has a width and a height that descibes the grid of nodes.  For example, the grid may be 4x4, and so there would be 16 nodes.\n",
    "\n",
    "Each node has a weight for each value in the input vector.  A weight is simply a float value that the node multiplies the input value by to determine how influential it is (see below)\n",
    "\n",
    "Each node has a set of weights that match the size of the input vector.  For example, if the input vector has 10 elements, each node would have 10 weights.\n",
    "\n",
    "### Training \n",
    "To train the network\n",
    "\n",
    "1. Each node's weights are initialized.\n",
    "2. We enumerate through the training data for some number of iterations (repeating if necessary).  The current value we are training against will be referred to as the `current input vector`\n",
    "3. Every node is examined to calculate which one's weights are most like the input vector. The winning node is commonly known as the Best Matching Unit (BMU).\n",
    "4. The radius of the neighbourhood of the BMU is now calculated. This is a value that starts large, typically set to the 'radius' of the lattice,  but diminishes each time-step. Any nodes found within this radius are deemed to be inside the BMU's neighbourhood.\n",
    "5. Each neighbouring node's (the nodes found in step 4) weights are adjusted to make them more like the input vector. The closer a node is to the BMU, the more its weights get altered.\n",
    "6. Go to step 2 until we've completed N iterations.\n",
    "    \n",
    "\n",
    "### Calculating the Best Matching Unit (BMU)\n",
    "\n",
    "To determine the best matching unit, one method is to iterate through all the nodes and calculate the Euclidean distance between each node's weight vector and the current input vector. The node with a weight vector closest to the input vector is tagged as the BMU.\n",
    "\n",
    "The Euclidean distance $\\mathsf{distance}_{i}$ (from the input vector $V$ to the $i$th node's weights $W_i$)is given as (using Pythagoras):\n",
    "\n",
    "$$ \\mathsf{distance}_{i}=\\sqrt{\\sum_{k=0}^{k=n}(V_k - W_{i_k})^2}$$\n",
    "\n",
    "where V is the current input vector and $W_i$ is the node's weight vector.  $n$ is the size of the input & weight vector.\n",
    "\n",
    "*Note*: $V$ and $W$ are vectors.  $V$ is the input vector, and $W_i$ is the weight vector of the $i$th node.  $V_k$ and $W_{i_k}$ represent the $k$'th value within those vectors.  \n",
    "\n",
    "The BMU is the node with the minimal distance for the current input vector\n",
    "\n",
    "### Calculating the Neighbourhood Radius\n",
    "\n",
    "The next step is to calculate which of the other nodes are within the BMU's neighbourhood. All these nodes will have their weight vectors altered.\n",
    "\n",
    "First we calculate what the radius of the neighbourhood should be and then use Pythagoras to determine if each node is within the radial distance or not.\n",
    "\n",
    "A unique feature of the Kohonen learning algorithm is that the area of the neighbourhood shrinks over time. To do this we use the exponential decay function:\n",
    "\n",
    "Given a desired number of training iterations $n$:\n",
    "$$n_{\\mathsf{max iterations}} = 100$$\n",
    "\n",
    "Calculate the radius $\\sigma_t$ at iteration number $t$:\n",
    "\n",
    "$$\\sigma_t = \\sigma_0 \\exp\\left(- \\frac{t}{\\lambda} \\right) \\qquad t = 1,2,3,4... $$\n",
    "\n",
    "Where $\\sigma_0$ denotes the neighbourhood radius at iteration $t=0$, $t$ is the current iteration. We define $\\sigma_0$ (the initial radius) and $\\lambda$ (the time constant) as below:\n",
    "\n",
    "$$\\sigma_0 = \\frac{\\max(width,height)}{2} \\qquad \\lambda = \\frac{n_{\\mathsf{max iterations}}}{\\log(\\sigma_0)} $$\n",
    "\n",
    "Where $width$ & $height$ are the width and height of the grid.\n",
    "\n",
    "### Calculating the Learning Rate\n",
    "\n",
    "We define the initial leanring rate $\\alpha_0$ at iteration $t = 0$ as:\n",
    "$$\\alpha_0 = 0.1$$\n",
    "\n",
    "So, we can calculate the learning rate at a given iteration t as:\n",
    "\n",
    "$$\\alpha_t = \\alpha_0 \\exp \\left(- \\frac{t}{\\lambda} \\right) $$\n",
    "\n",
    "where $t$ is the iteration number, $\\lambda$ is the time constant (calculated above)\n",
    "        \n",
    "### Calculating the Influence\n",
    "\n",
    "As well as the learning rate, we need to calculate the influence $\\theta_t$ of the learning/training at a given iteration $t$.  \n",
    "\n",
    "So for each node, we need to caclulate the euclidean distance $d_i$ from the BMU to that node.  Similar to when we calculate the distance to find the BMU, we use Pythagoras.  The current ($i$th) node's x position is given by $x(W_i)$, and the BMU's x position is, likewise, given by $x(Z)$.  Similarly, $y()$ returns the y position of a node.\n",
    "\n",
    "$$ d_{i}=\\sqrt{(x(W_i) - x(Z))^2 + (y(W_i) - y(Z))^2} $$\n",
    "\n",
    "Then, the influence decays over time according to:\n",
    "\n",
    "$$\\theta_t = \\exp \\left( - \\frac{d_{i}^2}{2\\sigma_t^2} \\right) $$\n",
    "\n",
    "Where $\\sigma_t$ is the neighbourhood radius at iteration $t$ as calculated above. \n",
    "\n",
    "Note: You will need to come up with an approach to x() and y().\n",
    "\n",
    "\n",
    "### Updating the Weights\n",
    "\n",
    "To update the weights of a given node, we use:\n",
    "\n",
    "$$W_{i_{t+1}} = W_{i_t} + \\alpha_t \\theta_t (V_t - W_{i_t})$$\n",
    "        \n",
    "So $W_{i_{t+1}}$ is the new value of the weight for the $i$th node, $V_t$ is the current value of the training data, $W_{i_t}$ is the current weight and $\\alpha_t$ and $\\theta_t$ are the learning rate and influence calculated above.\n",
    "\n",
    "*Note*: the $W$ and $V$ are vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Sam has written an implementation of a Self Organising Map. Consider the following criteria when assessing Sam's code:\n",
    "\n",
    "- Could the code be made more efficient? A literal interpretation of the instructions above is not necessary.\n",
    "  - Using too many python for loops. Must us numpy and vectors. This will also allow to use GPU in case that is available **Saad**\n",
    "- Is the code best structured for later use by other developers and in anticipation of productionisation?\n",
    "  - Nope, with this code it is very hard to understand what is happening at a glance\n",
    "    - Make a class\n",
    "    - Make internal functions for BMU, neighbourhood radius, learning rate and influence\n",
    "- How would you approach productionising this application?\n",
    "- Anything else you think is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kohonen.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(input_data, n_max_iterations, width, height, init_weights=None):\n",
    "    σ0 = max(width, height) / 2\n",
    "    α0 = 0.1\n",
    "    weights = (\n",
    "        init_weights\n",
    "        if init_weights is not None\n",
    "        else np.random.random((width, height, 3))\n",
    "    )\n",
    "    λ = n_max_iterations / np.log(σ0)\n",
    "    for t in range(n_max_iterations):\n",
    "        σt = σ0 * np.exp(-t / λ)\n",
    "        αt = α0 * np.exp(-t / λ)\n",
    "        for vt in input_data:\n",
    "            bmu = np.argmin(np.sum((weights - vt) ** 2, axis=2))\n",
    "            bmu_x, bmu_y = np.unravel_index(bmu, (width, height))\n",
    "            for x in range(width):\n",
    "                for y in range(height):\n",
    "                    di = np.sqrt(((x - bmu_x) ** 2) + ((y - bmu_y) ** 2))\n",
    "                    θt = np.exp(-(di**2) / (2 * (σt**2)))\n",
    "                    weights[x, y] += αt * θt * (vt - weights[x, y])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing efficiency \n",
    "\n",
    "- Use vector broad casting\n",
    "- numpy vector operations instead of loops\n",
    "- Update can be done without looping for x and y with meshgrid and broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_train(input_data, n_max_iterations, width, height, init_weights=None):\n",
    "    init_learning_rate = 0.1\n",
    "    init_radius = max(width, height) / 2\n",
    "    time_constant = n_max_iterations / np.log(init_radius)\n",
    "    \n",
    "    weights = (\n",
    "        init_weights\n",
    "        if init_weights is not None\n",
    "        else np.random.random((width, height, input_data.shape[-1]))\n",
    "    )\n",
    "    coord_x, coord_y = np.meshgrid(np.arange(width), np.arange(height), indexing=\"ij\")\n",
    "    for t in range(n_max_iterations):\n",
    "        current_radius = init_radius * np.exp(-t / time_constant)\n",
    "        current_learning_rate = init_learning_rate * np.exp(-t / time_constant)\n",
    "        for vt in input_data:\n",
    "            bmu = np.argmin(np.sum((weights - vt) ** 2, axis=2))\n",
    "            bmu_x, bmu_y = np.unravel_index(bmu, (width, height))\n",
    "\n",
    "            influence = np.sqrt((coord_x - bmu_x) ** 2 + (coord_y - bmu_y) ** 2)\n",
    "            influence_decay = np.exp(-(influence**2) / (2 * current_radius**2))\n",
    "            # broadcasting\n",
    "            influence_decay = influence_decay.reshape(\n",
    "                influence_decay.shape + (1,) * (weights.ndim - influence_decay.ndim)\n",
    "            )\n",
    "\n",
    "            weights += current_learning_rate * influence_decay * (vt - weights)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production - Kohonen Map\n",
    "\n",
    "- Logger class\n",
    "- Kohonen Map class\n",
    "- Model registry\n",
    "- Build, retraining, and inference pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger\n",
    "\n",
    "A `Telemetry` class which can log, logs to a jsonl file. It is a helper class, for the telemetry, so no need to look too much at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, logging\n",
    "\n",
    "class Telemetry:\n",
    "    def __init__(self, level=logging.INFO, log_path='.logs/telemetry.log'):\n",
    "        \"\"\"Initializes the telemetry system with a logger.\"\"\"\n",
    "        logger_name = 'TelemetryLogger'\n",
    "        if logger_name in logging.Logger.manager.loggerDict:\n",
    "            existing_logger = logging.getLogger(logger_name)\n",
    "            existing_logger.handlers.clear()\n",
    "        self.__logger = logging.getLogger(logger_name)\n",
    "        self.__logger.setLevel(level)\n",
    "        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "        handler = logging.FileHandler(log_path)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "        handler.setFormatter(formatter)\n",
    "        self.__logger.addHandler(handler)\n",
    "\n",
    "    def log(self, message: str, level=logging.INFO, **kwargs):\n",
    "        \"\"\"Logs a message with a given level.\"\"\"\n",
    "        self.__logger.log(level, message, extra=kwargs)\n",
    "\n",
    "    def error(self, error: Exception):\n",
    "        self.__logger.log(logging.ERROR, f\"[Training Error] f{str(error)}\", extra=dict(error=error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production worthy Kohonen Maps\n",
    "\n",
    "- Make is a class\n",
    "- Attach telemetry\n",
    "- The private methods must tell the steps in the algorithm\n",
    "- Use numpy boradcasts and vector algorithms for faster iterations\n",
    "- Add inference endpoint.\n",
    "- Add versioning for model code, param change, and retraining\n",
    "- Add checkpointing\n",
    "- Add data drift measurements (Improvement -> Instead of it being a part of model code make it a different class and put it some where else)\n",
    "    - Metric \n",
    "        - KL-Divergence\n",
    "        - Cost of matching the distribution (Wesstrine distance)\n",
    "        - Mean\n",
    "        - St deviation\n",
    "        - Skewness\n",
    "        - Kurtios (outlier detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os, pickle\n",
    "\n",
    "\n",
    "class KohonenMap:\n",
    "    @staticmethod\n",
    "    def current_code_version() -> int:\n",
    "        return 1 # Update this when you update the kohonen maps code\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    Implementation of a Kohonen Self-Organizing Map (SOM).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        width: int,\n",
    "        height: int,\n",
    "        input_dim: int,\n",
    "        model_version: int = 0, # Model build/ code version\n",
    "        model_last_revision: int = 0, # Model training version\n",
    "        max_iterations: int = 1000,\n",
    "        learning_rate: float = 0.1,\n",
    "        weights: Optional[np.ndarray] = None,\n",
    "        telemetry: Telemetry = Telemetry(),\n",
    "        checkpoint_dir: str = './.checkpoints'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Kohonen Map with the given dimensions and training parameters.\n",
    "\n",
    "        :param width: Width of the map.\n",
    "        :param height: Height of the map.\n",
    "        :param input_dim: Number of dimensions of the input vectors.\n",
    "        :param max_iterations: Maximum number of iterations for training.\n",
    "        :param learning_rate: Initial learning rate.\n",
    "        :param weights: Initial weights of the SOM. Randomly initialized if None.\n",
    "        :param telemetry: Telemetry object for logging.\n",
    "        :param checkpoint_dir: Directory where checkpoints will be saved.\n",
    "        \"\"\"\n",
    "        self.model_code_version: int = KohonenMap.current_code_version()\n",
    "        self.model_version = model_version\n",
    "        self.model_last_revision = model_last_revision\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = (\n",
    "            weights\n",
    "            if weights is not None\n",
    "            else np.random.random((width, height, input_dim))\n",
    "        )\n",
    "        assert self.weights.shape == (\n",
    "            width,\n",
    "            height,\n",
    "            input_dim,\n",
    "        ), f\"The weights must be of shape {(width, height, input_dim)}. The given is {self.weights.shape}\"\n",
    "        self.max_iterations = max_iterations\n",
    "        self.meshgrid = np.meshgrid(np.arange(width), np.arange(height), indexing=\"ij\")\n",
    "        self.init_radius = max(width, height) / 2\n",
    "        self.time_constant = max_iterations / np.log(self.init_radius)\n",
    "        self.telemetry = telemetry\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        self.training_data_stats = None\n",
    "\n",
    "    def __get_bmu(self, vector: np.ndarray) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Identifies the best matching unit (BMU) for a given input vector.\n",
    "\n",
    "        :param vector: Input vector.\n",
    "        :return: Tuple of indices for the BMU.\n",
    "        \"\"\"\n",
    "        self.telemetry.log(\n",
    "            \"Entered __get_bmu function\", logging.DEBUG, input=dict(vector=vector)\n",
    "        )\n",
    "        bmu = np.argmin(np.sum((self.weights - vector) ** 2, axis=2))\n",
    "        return np.unravel_index(bmu, (self.width, self.height))\n",
    "\n",
    "    def __calculate_influence(\n",
    "        self, bmu: Tuple[int, int], current_radius: float\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates the influence of the BMU over the map's neurons.\n",
    "\n",
    "        :param bmu: Best matching unit (BMU) indices.\n",
    "        :param current_radius: Current neighborhood radius.\n",
    "        :return: Influence matrix.\n",
    "        \"\"\"\n",
    "        self.telemetry.log(\n",
    "            \"Entered __calculate_influence function\",\n",
    "            logging.DEBUG,\n",
    "            input=dict(bmu=bmu, current_radius=current_radius),\n",
    "        )\n",
    "        bmu_x, bmu_y = bmu\n",
    "        coord_x, coord_y = self.meshgrid\n",
    "        influence = np.sqrt((coord_x - bmu_x) ** 2 + (coord_y - bmu_y) ** 2)\n",
    "        influence_decay = np.exp(-(influence**2) / (2 * current_radius**2))\n",
    "        return influence_decay.reshape(\n",
    "            influence_decay.shape + (1,) * (self.weights.ndim - influence_decay.ndim)\n",
    "        )\n",
    "\n",
    "    def __save_checkpoint(self, iteration: int):\n",
    "        \"\"\"\n",
    "        Saves a checkpoint of the current model state.\n",
    "\n",
    "        :param iteration: The current training iteration.\n",
    "        \"\"\"\n",
    "        checkpoint_file = os.path.join(\n",
    "            self.checkpoint_dir, f\"checkpoint_{self.model_code_version}.{self.model_version}.{self.model_last_revision}.{iteration}.pkl\"\n",
    "        )\n",
    "        checkpoint_data = {\n",
    "            \"iteration\": iteration,\n",
    "            \"weights\": self.weights,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"model_code_version\": self.model_code_version,\n",
    "            \"model_version\": self.model_version,\n",
    "            \"model_last_revision\": self.model_last_revision,\n",
    "        }\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(checkpoint_data, f)\n",
    "        self.telemetry.log(f\"Checkpoint saved at iteration {iteration}\", logging.INFO)\n",
    "\n",
    "    def __load_checkpoint(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Loads the most recent checkpoint if available.\n",
    "\n",
    "        :return: The iteration number from the checkpoint or None if no checkpoint exists.\n",
    "        \"\"\"\n",
    "        checkpoint_files = [f for f in os.listdir(self.checkpoint_dir) if f.startswith(f\"checkpoint_{self.model_code_version}.{self.model_version}.{self.model_last_revision}\")]\n",
    "        if not checkpoint_files:\n",
    "            return None\n",
    "        \n",
    "        checkpoint_files = list(map(lambda x: os.path.join(self.checkpoint_dir, x), checkpoint_files))\n",
    "        latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "        checkpoint_file = latest_checkpoint\n",
    "        \n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            checkpoint_data = pickle.load(f)\n",
    "        \n",
    "        self.weights = checkpoint_data[\"weights\"]\n",
    "        self.learning_rate = checkpoint_data[\"learning_rate\"]\n",
    "        self.model_code_version = checkpoint_data[\"model_code_version\"]\n",
    "        self.model_version = checkpoint_data[\"model_version\"]\n",
    "        self.model_last_revision = checkpoint_data[\"model_last_revision\"]\n",
    "        \n",
    "        iteration = checkpoint_data[\"iteration\"]\n",
    "        self.telemetry.log(f\"Loaded checkpoint from iteration {iteration}\", logging.INFO)\n",
    "        return iteration\n",
    "\n",
    "    def __delete_checkpoints(self):\n",
    "        \"\"\"\n",
    "        Deletes all checkpoint files in the checkpoint directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            checkpoint_files = [f for f in os.listdir(self.checkpoint_dir) if f.startswith(f\"checkpoint_{self.model_code_version}.{self.model_version}.{self.model_last_revision}\")]\n",
    "            for file_name in checkpoint_files:\n",
    "                os.remove(os.path.join(self.checkpoint_dir, file_name))\n",
    "            self.telemetry.log(\"All checkpoints have been deleted.\", logging.INFO)\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "\n",
    "    def __train(self, input_data: np.ndarray, validation_data: np.ndarray, training_checkpoint: int):\n",
    "        \"\"\"\n",
    "        Trains the Kohonen map using the provided input data.\n",
    "\n",
    "        :param input_data: Input data array.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            len(input_data.shape) == 2 and input_data.shape[-1] == self.input_dim\n",
    "        ), f\"The input_data must be of shape (N, {self.input_dim}). The given is {input_data.shape}\"\n",
    "\n",
    "        # Resume iterations just in case\n",
    "        start_iteration = self.__load_checkpoint() or 0\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            range(start_iteration, self.max_iterations),\n",
    "            total=self.max_iterations,\n",
    "            desc=\"Kohonen fitting iterations\",\n",
    "            initial=start_iteration\n",
    "        )\n",
    "\n",
    "        for iteration in progress_bar:\n",
    "            start_time = time.time()\n",
    "            ## Kohonene fitting - start\n",
    "            current_radius = self.init_radius * np.exp(-iteration / self.time_constant)\n",
    "            current_learning_rate = self.learning_rate * np.exp(\n",
    "                -iteration / self.time_constant\n",
    "            )\n",
    "            for vector in input_data:\n",
    "                bmu = self.__get_bmu(vector)\n",
    "                influence_decay = self.__calculate_influence(bmu, current_radius)\n",
    "                self.weights += (\n",
    "                    current_learning_rate * influence_decay * (vector - self.weights)\n",
    "                )\n",
    "            ## Kohonen fitting - end\n",
    "            if iteration % training_checkpoint == 0:\n",
    "                self.__save_checkpoint(iteration)\n",
    "                inference = self.infer_with_metrics(validation_data)\n",
    "                self.telemetry.log(f\"Iteration {iteration}/{self.max_iterations} - {inference['log_string']}\")\n",
    "                progress_bar.set_postfix(validation=inference['log_string'])\n",
    "                \n",
    "            elapsed_time = time.time() - start_time\n",
    "            self.telemetry.log(\n",
    "                f\"Iteration {iteration}/{self.max_iterations} complete in {elapsed_time:.5f}s\",\n",
    "                level=logging.DEBUG,\n",
    "                params=dict(\n",
    "                    iteration=iteration,\n",
    "                    total_iterations=self.max_iterations,\n",
    "                    kohonen_map_size=(self.width, self.height),\n",
    "                    input_data_shape=input_data.shape,\n",
    "                    init_radius=self.init_radius,\n",
    "                    time_constant=self.time_constant,\n",
    "                    current_radius=current_radius,\n",
    "                    current_learning_rate=current_learning_rate,\n",
    "                    elapsed_time=elapsed_time,\n",
    "                ),\n",
    "            )\n",
    "        self.__delete_checkpoints()\n",
    "        return self.weights\n",
    "\n",
    "    def train(self, input_data: np.ndarray, validation_data_size: float = 0.2 ,training_checkpoint: int = 100):\n",
    "        \"\"\"\n",
    "        Public method to start the training process.\n",
    "\n",
    "        :param input_data: Input data array.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.telemetry.log(\n",
    "                \"Entered train function\",\n",
    "                level=logging.DEBUG,\n",
    "                input=dict(\n",
    "                    input_data_shape=input_data.shape,\n",
    "                    training_checkpoint=training_checkpoint,\n",
    "                ),\n",
    "            )\n",
    "            start_time = time.time()\n",
    "            \n",
    "            validation_samples_count = int(input_data.shape[0] * validation_data_size)\n",
    "            random_indices = np.random.choice(input_data.shape[0], validation_samples_count, replace=False)\n",
    "            validation_data = input_data[random_indices]\n",
    "\n",
    "            result = self.__train(input_data, validation_data, training_checkpoint)\n",
    "            self.model_last_revision += 1\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            self.telemetry.log(f\"Training completed in {elapsed_time}s\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "            raise e\n",
    "            \n",
    "\n",
    "    def infer(self, input_data: np.ndarray) -> Tuple[int, int, float]:\n",
    "        \"\"\"\n",
    "        Infers the best matching unit (BMU) for a given input vector.\n",
    "\n",
    "        :param input_vector: Input vector to locate the BMU for.\n",
    "        :return: Tuple of indices representing the position of the BMU and the eucleadian distance for the bmu.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.telemetry.log(\n",
    "                f\"Entered infer function with model ({self.model_version}.{self.model_last_revision})\",\n",
    "                level=logging.DEBUG,\n",
    "                input=dict(input_data_shape=input_data.shape),\n",
    "            )\n",
    "            assert len(input_data.shape) == 2 and input_data.shape[-1] == self.input_dim, f'The input vector MUST be of shape (N, {self.input_dim}). The provided is f{input_data.shape}'\n",
    "            result: list[Tuple[int, int, float]] = []\n",
    "            for vector in input_data:\n",
    "                bmu_index = np.argmin(np.linalg.norm(self.weights - vector, axis=2))\n",
    "                bmu_coords = np.unravel_index(bmu_index, (self.width, self.height))\n",
    "                euclidean_distance = np.linalg.norm(self.weights[bmu_coords] - vector)\n",
    "                result.append((*bmu_coords, euclidean_distance))\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "            raise e\n",
    "        \n",
    "    def infer_with_metrics(self, input_data: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Performs inference on the given input data and computes metrics.\n",
    "\n",
    "        This method calculates the best matching unit (BMU) for each input vector in the\n",
    "        provided dataset and computes the Euclidean distance between the input vectors\n",
    "        and their corresponding BMUs. It returns a dictionary containing the inferences,\n",
    "        the mean distance error, and a log string summarizing the mean error.\n",
    "\n",
    "        :param input_data: A numpy array of shape (N, D) where N is the number of input \n",
    "                        vectors and D is the dimensionality of each vector.\n",
    "\n",
    "        :return: A dictionary with the following keys:\n",
    "                - 'inferences': A list of tuples, each containing the BMU coordinates \n",
    "                                (x, y) and the Euclidean distance for each input vector.\n",
    "                - 'mean_error': The mean Euclidean distance between the input vectors \n",
    "                                and their corresponding BMUs.\n",
    "                - 'log_string': A formatted string summarizing the mean distance error.\n",
    "        \"\"\"\n",
    "        inferences = self.infer(input_data)\n",
    "        errors: list[float] = list(map(lambda x: x[-1], inferences))\n",
    "        mean_distance = sum(errors) / len(errors)\n",
    "        log_string = f\"Mean distance: {mean_distance:.4f}\"\n",
    "        return dict(\n",
    "            inferences=inferences,\n",
    "            mean_error = mean_distance,\n",
    "            log_string = log_string,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Resgistry\n",
    "\n",
    "Store the model based on its version, environment, and revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class KohonenMapRegistry:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        environment: str = 'dev',\n",
    "        artefact_base_name: str = 'kohonen',\n",
    "        telemetry: Telemetry = Telemetry()\n",
    "    ):\n",
    "        # Registery directory\n",
    "        self.__dir_path = os.path.join(dir_path, environment)\n",
    "        self.environment = environment\n",
    "        self.__artefact_base_name = artefact_base_name\n",
    "        self.telemetry = telemetry\n",
    "        os.makedirs(self.__dir_path, exist_ok=True)\n",
    "        \n",
    "\n",
    "    def save(self, kohonenMap: KohonenMap):\n",
    "        try: \n",
    "            version = kohonenMap.model_version\n",
    "            revision = kohonenMap.model_last_revision\n",
    "            code_version = kohonenMap.model_code_version\n",
    "            model_artefact = dict(\n",
    "                width=kohonenMap.width,\n",
    "                height=kohonenMap.height,\n",
    "                input_dim=kohonenMap.input_dim,\n",
    "                max_iterations=kohonenMap.max_iterations,\n",
    "                learning_rate=kohonenMap.learning_rate,\n",
    "                weights=kohonenMap.weights,\n",
    "                build_version=version,\n",
    "                code_version=code_version,\n",
    "                training_revision=revision,\n",
    "                execution_environment=self.environment,\n",
    "            )\n",
    "            model_artefact_file_name = f\"{self.__artefact_base_name}-{self.environment}-{code_version}.{version}.{revision}.pkl\"\n",
    "            file_path = os.path.join(self.__dir_path, model_artefact_file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                raise FileExistsError(f\"The file {file_path} already exists.\")\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(model_artefact, f)\n",
    "            self.telemetry.log(f\"Model artefact saved at {file_path}\")\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "            raise e\n",
    "        \n",
    "    def load(self, code_version: int, version: int, revision: int) -> KohonenMap:\n",
    "        try:\n",
    "            model_artefact_file_name = f\"{self.__artefact_base_name}-{self.environment}-{code_version}.{version}.{revision}.pkl\"\n",
    "            file_path = os.path.join(self.__dir_path, model_artefact_file_name)\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "            with open(file_path, 'rb') as f:\n",
    "                model_artefact = pickle.load(f)\n",
    "            kohonenMap = KohonenMap(\n",
    "                width=model_artefact['width'],\n",
    "                height=model_artefact['height'],\n",
    "                input_dim=model_artefact['input_dim'],\n",
    "                max_iterations=model_artefact['max_iterations'],\n",
    "                learning_rate=model_artefact['learning_rate'],\n",
    "                weights=model_artefact['weights'],\n",
    "                model_version=version,\n",
    "                model_last_revision=revision,\n",
    "                telemetry=self.telemetry\n",
    "            )\n",
    "            self.telemetry.log(f\"Model artefact loaded from {file_path}\")\n",
    "            return kohonenMap\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "            raise e\n",
    "\n",
    "    def list(self) -> list[Tuple[int, int, int]]:\n",
    "        try:\n",
    "            artefacts = []\n",
    "            for file_name in os.listdir(self.__dir_path):\n",
    "                if file_name.startswith(self.__artefact_base_name) and file_name.endswith('.pkl'):\n",
    "                    parts = file_name.split('-')\n",
    "                    env, version_revision = parts[1], parts[2]\n",
    "                    if env == self.environment:\n",
    "                        code_version, version, revision = map(int, version_revision.split('.')[0:3])\n",
    "                        artefacts.append((code_version, version, revision))\n",
    "            \n",
    "            artefacts.sort(reverse=True, key=lambda x: (x[0], x[1], x[2]))\n",
    "            return artefacts\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "            raise e\n",
    "    \n",
    "    def load_latest(self, code_version: Optional[int] = None, version: Optional[int] = None) -> Optional[KohonenMap]:\n",
    "        try:\n",
    "            if (code_version is None and version is not None) or (code_version is not None and version is None):\n",
    "                raise ValueError(\"Either both code_version and version should be None or both should be provided.\")\n",
    "\n",
    "            artefacts = self.list()\n",
    "\n",
    "            if code_version is None and version is None:\n",
    "                if not artefacts:\n",
    "                    self.telemetry.log(\"No models found in the registry.\")\n",
    "                    return None\n",
    "                latest_code_version, latest_version, latest_revision = artefacts[0]\n",
    "            else:\n",
    "                # Specific code_version and version provided, load the latest revision for these\n",
    "                filtered_artefacts = [\n",
    "                    (cv, v, r) for (cv, v, r) in artefacts if cv == code_version and v == version\n",
    "                ]\n",
    "                if not filtered_artefacts:\n",
    "                    self.telemetry.log(f\"No models found for code_version={code_version}, version={version}.\")\n",
    "                    return None\n",
    "                latest_code_version, latest_version, latest_revision = filtered_artefacts[0]\n",
    "\n",
    "            self.telemetry.log(f\"Loading latest model: code_version={latest_code_version}, version={latest_version}, revision={latest_revision}\")\n",
    "            return self.load(latest_code_version, latest_version, latest_revision)\n",
    "        except Exception as e:\n",
    "            self.telemetry.error(e)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train/ Retraining pipeline\n",
    "\n",
    "- Init Model registry for environment\n",
    "- Update the model code\n",
    "- Set the parameters\n",
    "    - input dimensions\n",
    "    - width\n",
    "    - height\n",
    "    - learning rate\n",
    "    - **Note:** Updating any of the above must cause a model version update\n",
    "- Search for an existing model revision in the existing directory, if it does not exist then the model is brand new\n",
    "- Train it. Preserve checkpoint in case of loading again\n",
    "- Test the model\n",
    "- Save the model in the registery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagram explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%plantuml\n",
    "\n",
    "@startuml\n",
    "title Model Train/Retraining Pipeline\n",
    "\n",
    "start\n",
    "\n",
    ":Initialized Model Registry for Environment;\n",
    ":Update Model Code;\n",
    ":Set Parameters;\n",
    "note right\n",
    "  - Input Dimensions\n",
    "  - Width\n",
    "  - Height\n",
    "  - Learning Rate\n",
    "  **Note:** Updating any of the above \n",
    "  must cause a model version update\n",
    "end note\n",
    "\n",
    ":Search for Existing Model Revision in Registry;\n",
    "if (Model Revision Exists?) then (Yes)\n",
    "  :Load Existing Model;\n",
    "else (No)\n",
    "  :Initialize New Model;\n",
    "endif\n",
    "\n",
    ":Train Model;\n",
    "note right\n",
    "  Preserve checkpoints \n",
    "  in case of loading again\n",
    "end note\n",
    "\n",
    ":Test Model;\n",
    "\n",
    ":Save Model in Registry;\n",
    "\n",
    "stop\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training data\n",
    "training_samples = 1000\n",
    "training_data = np.random.random((training_samples, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kohonen fitting iterations:  79%|███████▉  | 788/1000 [03:07<03:20,  1.06it/s, validation=Mean distance: 0.4196]"
     ]
    }
   ],
   "source": [
    "telemetry = Telemetry(level=logging.INFO)\n",
    "\n",
    "registry = KohonenMapRegistry('registry', environment='dev', telemetry=telemetry)\n",
    "\n",
    "MODEL_VERSION = 2\n",
    "\n",
    "# must update MODEL_VERSION for these\n",
    "width = 100\n",
    "height = 100\n",
    "learning_rate = 0.1\n",
    "input_dims = 10\n",
    "\n",
    "# No need to update MODEL_VERSION for these\n",
    "max_iterations = 1000\n",
    "training_checkpoint = int(max_iterations * 0.1)\n",
    "\n",
    "last_revision = registry.load_latest(\n",
    "  code_version=KohonenMap.current_code_version(), \n",
    "  version=MODEL_VERSION\n",
    ")\n",
    "\n",
    "if last_revision:\n",
    "    kmap = last_revision\n",
    "else:\n",
    "    kmap = KohonenMap(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        learning_rate=learning_rate,\n",
    "        input_dim=input_dims,\n",
    "        model_version=MODEL_VERSION,\n",
    "        model_last_revision= 0,\n",
    "        max_iterations=max_iterations,\n",
    "        telemetry=telemetry\n",
    "    )\n",
    "\n",
    "kmap.train(training_data, training_checkpoint=training_checkpoint)\n",
    "\n",
    "# TODO: Test model\n",
    "\n",
    "registry.save(kmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference pipeline\n",
    "\n",
    "The inference pipline is used for inferences. It has the following steps. \n",
    "\n",
    "- Init the environment registery\n",
    "- Fetch the latest model\n",
    "- Run it (In this case, I am going to run it by testing 10 data points and print the image of the map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Kohonen Map (1.2.1)\n",
      "Inference metrics = Mean distance: 0.6830\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_point</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.2739084982088328, 0.07544762398438565, 0.97...</td>\n",
       "      <td>99</td>\n",
       "      <td>39</td>\n",
       "      <td>0.670576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.7951659895467311, 0.056887516002243954, 0.6...</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>0.630497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.7562891233283073, 0.2839030392245179, 0.568...</td>\n",
       "      <td>61</td>\n",
       "      <td>39</td>\n",
       "      <td>0.640769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.867879744328657, 0.5252548386347248, 0.6741...</td>\n",
       "      <td>72</td>\n",
       "      <td>94</td>\n",
       "      <td>0.640558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.9266206880972113, 0.23106056398274377, 0.44...</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>0.800641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.6660640259832218, 0.7372899113884959, 0.072...</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>0.763954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.39321978867224894, 0.9003664001965379, 0.56...</td>\n",
       "      <td>49</td>\n",
       "      <td>90</td>\n",
       "      <td>0.857962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.6496704044812844, 0.8481576502231856, 0.539...</td>\n",
       "      <td>54</td>\n",
       "      <td>60</td>\n",
       "      <td>0.514961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.9281284207665963, 0.9896349878071095, 0.507...</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0.746676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.4897382694289524, 0.1966524094055918, 0.566...</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>0.563075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          data_point   x   y  distance\n",
       "0  [0.2739084982088328, 0.07544762398438565, 0.97...  99  39  0.670576\n",
       "1  [0.7951659895467311, 0.056887516002243954, 0.6...  44  29  0.630497\n",
       "2  [0.7562891233283073, 0.2839030392245179, 0.568...  61  39  0.640769\n",
       "3  [0.867879744328657, 0.5252548386347248, 0.6741...  72  94  0.640558\n",
       "4  [0.9266206880972113, 0.23106056398274377, 0.44...  11  45  0.800641\n",
       "5  [0.6660640259832218, 0.7372899113884959, 0.072...  43  43  0.763954\n",
       "6  [0.39321978867224894, 0.9003664001965379, 0.56...  49  90  0.857962\n",
       "7  [0.6496704044812844, 0.8481576502231856, 0.539...  54  60  0.514961\n",
       "8  [0.9281284207665963, 0.9896349878071095, 0.507...  15   5  0.746676\n",
       "9  [0.4897382694289524, 0.1966524094055918, 0.566...  11  39  0.563075"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "telemetry = Telemetry(level=logging.ERROR)\n",
    "\n",
    "registry = KohonenMapRegistry('registry', environment='dev', telemetry=telemetry)\n",
    "\n",
    "kmap = registry.load_latest()\n",
    "if kmap is None:\n",
    "  raise Exception(\"No valid model available in registry\")\n",
    "\n",
    "print(f\"Using Kohonen Map ({kmap.model_code_version}.{kmap.model_version}.{kmap.model_last_revision})\")\n",
    "\n",
    "# Defining some inference data\n",
    "inference_input_dims = kmap.input_dim\n",
    "infernce_samples = 10\n",
    "inference_data = np.random.random((infernce_samples, inference_input_dims))\n",
    "\n",
    "result = kmap.infer_with_metrics(inference_data)\n",
    "inference = result['inferences']\n",
    "print(f\"Inference metrics = {result['log_string']}\")\n",
    "\n",
    "# Displaying data\n",
    "if kmap.weights.shape[-1] == 3:\n",
    "    image_path = f\"images/kohonen-{registry.environment}-{kmap.model_code_version}.{kmap.model_version}.{kmap.model_last_revision}.png\"\n",
    "    plt.imsave(image_path, kmap.weights)\n",
    "    log_message = f\"Weights image saved at {image_path}\"\n",
    "    telemetry.log(log_message)\n",
    "    print(log_message)\n",
    "\n",
    "data: list[Tuple[np.ndarray, Tuple[int, int, float]]] = list(zip(inference_data, inference))\n",
    "df = pd.DataFrame(data, columns=['data_point', 'coordinates'])\n",
    "df[['x', 'y', 'distance']] = pd.DataFrame(df['coordinates'].tolist(), index=df.index)\n",
    "df.drop(columns=['coordinates'], inplace=True)\n",
    "df\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
